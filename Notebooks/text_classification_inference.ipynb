{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbojEZaxsehr",
        "outputId": "6378d2ab-34bd-41e9-d078-749c6a786c0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/document_classifier/\")"
      ],
      "metadata": {
        "id": "8vxbEzHOsiEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pymupdf"
      ],
      "metadata": {
        "id": "lbJZEO-Ev7Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# import fitz\n",
        "# filepath = \"./a2937214-5676-4354-a85c-8c26580b46ab.pdf\"\n",
        "\n",
        "# text = ''\n",
        "# with fitz.open(filepath ) as doc:\n",
        "#     for page in doc:\n",
        "#         text+= page.get_text(\"text\")\n",
        "# print(text)"
      ],
      "metadata": {
        "id": "jEjorUgOynl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os, re\n",
        "# from pdf2image import convert_from_path\n",
        "import string\n",
        "import nltk\n",
        "import fitz\n",
        "nltk.download('stopwords')\n",
        "# from multiprocessing import Pool\n",
        "\n",
        "def load_model(filename):\n",
        "  # load the model from disk\n",
        "  loaded_model = pickle.load(open(filename, 'rb'))\n",
        "  return loaded_model\n",
        "\n",
        "loaded_model = load_model(filename='./finalized_model.pkl')\n",
        "\n",
        "punct =[]\n",
        "punct += list(string.punctuation)\n",
        "punct += '’'\n",
        "punct.remove(\"'\")\n",
        "def remove_punctuations(text):\n",
        "    for punctuation in punct:\n",
        "        text = text.replace(punctuation, ' ')\n",
        "    return text\n",
        "\n",
        "#Stop words present in the library\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "#defining the function to remove stopwords from tokenized text\n",
        "def remove_stopwords(text):\n",
        "    tokens = text.split()\n",
        "    output= \" \".join([i for i in tokens if i not in stopwords])\n",
        "    return output\n",
        "\n",
        "# def get_text_from_page(page_number, page):\n",
        "#   # page = read_pdf.getPage(page_number)\n",
        "#   # page_content = page.extractText()\n",
        "#   page_content = page.get_text(\"text\")\n",
        "#   print(page_content)\n",
        "\n",
        "#   #Text cleaning\n",
        "#   page_content = remove_punctuations(page_content)\n",
        "#   page_content = re.sub(' +', ' ', page_content)\n",
        "#   page_content = remove_stopwords(page_content)\n",
        "#   page_content = page_content.lower()\n",
        "\n",
        "#   return (page_number+1, page_content)\n",
        "\n",
        "\n",
        "def get_classification(pdf_path):\n",
        "    #Extract text from each page\n",
        "    page_info = []\n",
        "    result = {}\n",
        "    target_name = ['not_important', 'important']\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "      for i, page in enumerate(doc):\n",
        "          page_info.append((i+1, page.get_text(\"text\")))\n",
        "    txt_list = [str(txt) for page, txt in page_info]\n",
        "\n",
        "    # Text cleaning\n",
        "    txt_process = list(map(remove_punctuations, txt_list))\n",
        "    txt_process = list(map(lambda x: str(x).replace(\"  \", \" \"), txt_process))\n",
        "    txt_process = list(map(remove_stopwords, txt_process))\n",
        "    final_txt_list = list(map(lambda x: x.lower(), txt_process))\n",
        "\n",
        "    #model prediction\n",
        "    pred_prob = loaded_model.predict_proba(final_txt_list)\n",
        "    ix = pred_prob.argmax(1)\n",
        "    prediction = [(target_name[val], round(float(prob[val])*100, 2), info[0]) for val, prob, info in zip(ix, pred_prob, page_info)]\n",
        "\n",
        "    result['important'] = [{'page_number': page, 'confidence_score': conf} for label, conf, page in prediction if label == 'important']\n",
        "    result['not_important'] = [{'page_number': page, 'confidence_score': conf} for label, conf, page in prediction if label == 'not_important']\n",
        "\n",
        "    return {'result': result}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZXfh6sUPJ4U",
        "outputId": "4f948c22-a2d6-496e-c013-6ddfc657e7af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "result = get_classification(\"./5ee3f827-6ff8-40b2-b4a3-96d1d25325d4.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuxW7DfqsfFW",
        "outputId": "e53f30a2-dfcd-448b-ed03-16ab13a6178c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 102 ms, sys: 6.84 ms, total: 109 ms\n",
            "Wall time: 165 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvBUPw7vT_LX",
        "outputId": "bf78f931-d130-4d06-9126-c8526b2839cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'result': {'important': [{'page_number': 4, 'confidence_score': 100.0}],\n",
              "  'not_important': [{'page_number': 1, 'confidence_score': 86.81},\n",
              "   {'page_number': 2, 'confidence_score': 99.27},\n",
              "   {'page_number': 3, 'confidence_score': 98.61},\n",
              "   {'page_number': 5, 'confidence_score': 99.73},\n",
              "   {'page_number': 6, 'confidence_score': 96.07}]}}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FPkj88Ct8aLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zi4A_Jlh8aHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uGvKx5wk8aFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UyRHoDBU8aCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4PBjZfBz8Z_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wn-YGEY88Z85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pdf2image\n",
        "# !sudo apt-get install poppler-utils\n",
        "# !pip install PyPDF2"
      ],
      "metadata": {
        "id": "WvYvP3iJsfAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_classification(pdf_path):\n",
        "#     #Extract text from each page\n",
        "#     # read_pdf = PyPDF2.PdfFileReader(pdf_path)\n",
        "#     # number_of_pages = read_pdf.getNumPages()\n",
        "#     page_info = []\n",
        "#     result = {}\n",
        "#     target_name = ['not_important', 'important']\n",
        "#     result_objs = []\n",
        "#     doc = fitz.open(pdf_path)\n",
        "#     #Using multiprocessing for extracting text from multiple pages in a pdf file.\n",
        "#     print(\"Total Number of CPU cores :\", os.cpu_count())\n",
        "#     with Pool(processes=os.cpu_count() - 1) as pool:\n",
        "#         # for i, page_number in enumerate(range(number_of_pages)):   # use xrange in Py2\n",
        "#         # with fitz.open(pdf_path) as doc:\n",
        "#           for i, page in enumerate(doc):\n",
        "#               output = pool.apply_async(get_text_from_page, (i, page,))\n",
        "#               result_objs.append(output)\n",
        "#               print(output.get)\n",
        "#           page_info = [res.get() for res in result_objs]\n",
        "#           final_txt_list = [str(txt) for page, txt in page_info]\n",
        "#     doc.close()\n",
        "#     # Text cleaning\n",
        "#     # txt_process = list(map(remove_punctuations, txt_list))\n",
        "#     # txt_process = list(map(lambda x: str(x).replace(\"  \", \" \"), txt_process))\n",
        "#     # txt_process = list(map(remove_stopwords, txt_process))\n",
        "#     # final_txt_list = list(map(lambda x: x.lower(), txt_process))\n",
        "\n",
        "#     #model prediction\n",
        "#     pred_prob = loaded_model.predict_proba(final_txt_list)\n",
        "#     ix = pred_prob.argmax(1)\n",
        "#     prediction = [(target_name[val], round(float(prob[val])*100, 2), info[0]) for val, prob, info in zip(ix, pred_prob, page_info)]\n",
        "\n",
        "#     result['important'] = [{'page_number': page, 'confidence_score': conf} for label, conf, page in prediction if label == 'important']\n",
        "#     result['not_important'] = [{'page_number': page, 'confidence_score': conf} for label, conf, page in prediction if label == 'not_important']\n",
        "\n",
        "#     return {'result': result}\n"
      ],
      "metadata": {
        "id": "BIAwhdtO08TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pdf_path = './5ee3f827-6ff8-40b2-b4a3-96d1d25325d4.pdf'\n",
        "# import PyPDF2, os, re\n",
        "# from pdf2image import convert_from_path\n",
        "# filename = pdf_path.split(\"/\")[-1].split(\".\")[0]\n",
        "# if not os.path.exists(f\"./media/{filename}\"):\n",
        "#     os.makedirs(f\"./media/{filename}\")\n",
        "# images = convert_from_path(pdf_path)\n",
        "# read_pdf = PyPDF2.PdfFileReader(pdf_path)\n",
        "# number_of_pages = read_pdf.getNumPages()\n",
        "# for i, page_number in enumerate(range(number_of_pages)):   # use xrange in Py2\n",
        "#     page = read_pdf.getPage(page_number)\n",
        "#     page_content = page.extractText()\n",
        "#     images[i].save(f\"./media/{filename}/\"+ filename+ '_page_'+ str(i) +'.jpg', 'JPEG')\n",
        "#     file1 = open(f\"./media/{filename}/\"+ filename+ '_page_'+ str(i) +'.txt',\"w\")#append mode\n",
        "#     page_content = re.sub(' +', ' ', page_content)\n",
        "#     file1.write(page_content)\n",
        "#     file1.close()"
      ],
      "metadata": {
        "id": "vsH6EZ3Ps1Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image_list, txt_list = [], []\n",
        "# for file in os.listdir(f\"./media/{filename}/\"):\n",
        "#   if file.endswith('.jpg'):\n",
        "#       image_list.append(f\"./media/{filename}/\" + file)\n",
        "#       f = open(f\"./media/{filename}/\" + file.replace(\".jpg\", '.txt'), 'r')\n",
        "#       txt =  f.read()\n",
        "#       txt_list.append(txt)\n",
        "#       f.close()"
      ],
      "metadata": {
        "id": "hLxoFO4Qs7c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import string\n",
        "\n",
        "# punct =[]\n",
        "# punct += list(string.punctuation)\n",
        "# punct += '’'\n",
        "# punct.remove(\"'\")\n",
        "# def remove_punctuations(text):\n",
        "#     for punctuation in punct:\n",
        "#         text = text.replace(punctuation, ' ')\n",
        "#     return text"
      ],
      "metadata": {
        "id": "AIK8ZuBBs9oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# #Stop words present in the library\n",
        "# stopwords = nltk.corpus.stopwords.words('english')\n",
        "# #defining the function to remove stopwords from tokenized text\n",
        "# def remove_stopwords(text):\n",
        "#     tokens = text.split()\n",
        "#     output= \" \".join([i for i in tokens if i not in stopwords])\n",
        "#     return output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKdWNAratAgl",
        "outputId": "6bd21476-f3b9-42c3-eb5d-e825e20905b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# txt_ = list(map(remove_punctuations, txt_list))\n",
        "# txt_ = list(map(lambda x: str(x).replace(\"  \", \" \"), txt_))\n",
        "# txt_ = list(map(remove_stopwords, txt_))\n",
        "# final_txt_list = list(map(lambda x: x.lower(), txt_))"
      ],
      "metadata": {
        "id": "yPSpESYYtDYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final_txt_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "IERhKaWRtFlj",
        "outputId": "22b7d037-f0fb-4448-8850-a9be31c51f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'insured details partner details name partner name maheshbhai patel invictus insurance broking services private ltd address partner code 62 tanki faliyu italva navsari gujarat 396445 db56713 mobile number partner mobile number 9106453742 9513312901 policy no partner email 3001 241154739 00 000 hello maheshbhai patel car covered here private car package policy product code 3001 uin irdan115rp0017v01200102 ref no w136545691 date feb 28 2022 corp sup opi 2014 1777'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predicted = loaded_model.predict(final_txt_list)"
      ],
      "metadata": {
        "id": "wHkn43wBtHeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEyS6t4VtKTn",
        "outputId": "b6fccaad-b47a-4731-efd5-06e4806fdfab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# target_name = ['not_important', 'important']\n",
        "# pred_prob = loaded_model.predict_proba(final_txt_list)\n",
        "# ix = pred_prob.argmax(1)\n",
        "# prediction = [(target_name[val], round(prob[val], 2)) for val, prob in zip(ix, pred_prob)]\n",
        "# print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPQ4x99jtMRL",
        "outputId": "c1abda95-6463-4a1f-96b4-202175b158de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('not_important', 0.87), ('not_important', 0.99), ('not_important', 0.98), ('important', 1.0), ('not_important', 1.0), ('not_important', 0.95)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ARJGQBcotPVF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}